{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***ToMe***\n",
        "\n",
        "ViT converts image patches into “tokens,” then applies an attention mechanism in each layer that allows these tokens to collect information from one another, proportional to their similarity. To improve the speed of ViT while maintaining its accuracy, ToMe takes redundant tokens and merges them based on similarity, reducing the number of tokens without losing information.\n",
        "\n",
        "During inference, ToMe lowers the number of tokens gradually over the course of the network, significantly reducing the overall time taken.\n"
      ],
      "metadata": {
        "id": "U6xP8YcuoBQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Installing dependencies***"
      ],
      "metadata": {
        "id": "KqETc5ZHhMv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets --quiet"
      ],
      "metadata": {
        "id": "uvChtYCZjSTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install git+https://github.com/facebookresearch/ToMe.git --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqpkZZtX3G0b",
        "outputId": "0481ba2c-655d-4316-8aba-797e54ca729e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Validation***"
      ],
      "metadata": {
        "id": "ioK25ffRoz3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre trained model without token merging"
      ],
      "metadata": {
        "id": "6eoZW0QVf2-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pretrained ViT is loaded and an image of a dog is passed through the network and get predictions for the resulting classes."
      ],
      "metadata": {
        "id": "hoOdV8mSdaRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import tome\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Load a pretrained model\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "\n",
        "input_size = model.default_cfg[\"input_size\"][1]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(int((256 / 224) * input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(model.default_cfg[\"mean\"], model.default_cfg[\"std\"]),\n",
        "])\n",
        "\n",
        "img = Image.open(\"images/husky.png\")\n",
        "img_tensor = transform(img)[None, ...]\n",
        "\n",
        "model(img_tensor).topk(5).indices[0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc483ane9fJl",
        "outputId": "57f654a0-01ff-411a-a623-97b5b57bd36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[248, 250, 249, 537, 174]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre trained model with token merging\n"
      ],
      "metadata": {
        "id": "8B7MZRcJf1fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the ToMe mechanism is applied but with no reduction. As we can see the predicted classes remain the same."
      ],
      "metadata": {
        "id": "ffzEUwIjYp-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch the model with ToMe\n",
        "tome.patch.timm(model)\n",
        "\n",
        "# Run the model with no reduction (should be the same as before)\n",
        "model.r = 0\n",
        "model(img_tensor).topk(5).indices[0].tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0huzcEGgMcB",
        "outputId": "4194b3a7-1913-4a99-a99b-04a9fbd6df05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[248, 250, 249, 537, 174]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, reducing the number of token per layer, leaves unchanged the first three predicted classes. This for both r = 8 and r = 16."
      ],
      "metadata": {
        "id": "ISNwX4iGXP8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model with some reduction\n",
        "model.r = 8\n",
        "model(img_tensor).topk(5).indices[0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8VN3YunlPdB",
        "outputId": "adfd1df1-96ac-42eb-a0f3-a4af5b1a54ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[248, 250, 249, 269, 537]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model with a lot of reduction\n",
        "# Top-3 most applicable classes didn't change (husky, Siberian husky, Alaskan malamute)\n",
        "# But model is 2x faster now! See benchmarking section.\n",
        "model.r = 16\n",
        "model(img_tensor).topk(5).indices[0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CHSZtetlQVR",
        "outputId": "98809f97-c6cf-4f33-9f52-b013e76d3653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[248, 250, 249, 269, 537]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Benchmarking***"
      ],
      "metadata": {
        "id": "RDwLEfsH9R0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The followign is the benchmark of the vision transformers without token merging\n"
      ],
      "metadata": {
        "id": "P6XHd4M89_YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import tome\n",
        "\n",
        "\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "\n",
        "device = \"cuda:0\"\n",
        "runs = 50\n",
        "batch_size = 256\n",
        "input_size = model.default_cfg[\"input_size\"]\n",
        "\n",
        "baseline_throughput = tome.utils.benchmark(\n",
        "    model,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        "    runs=runs,\n",
        "    batch_size=batch_size,\n",
        "    input_size=input_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nil5_Tto7dZq",
        "outputId": "2b642034-72b8-4d9b-9fcd-c9df403786dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Benchmarking: 100%|██████████| 50/50 [02:18<00:00,  2.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 84.07 im/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is the benchmark of the vision transformer with merging layer. We merge tokens with a constant schedule, i.e. r= 8 and r = 16 per layer. This increases almost x2 the throughput of the network."
      ],
      "metadata": {
        "id": "9TFzywkM-Id7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tome.patch.timm(model)\n",
        "\n",
        "model.r = 8\n",
        "tome_throughput = tome.utils.benchmark(\n",
        "    model,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        "    runs=runs,\n",
        "    batch_size=batch_size,\n",
        "    input_size=input_size\n",
        ")\n",
        "print(f\"Throughput improvement: {tome_throughput / baseline_throughput:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGNQkHSEfziG",
        "outputId": "582b34b9-63b2-403a-97b1-dc25317439a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Benchmarking: 100%|██████████| 50/50 [01:49<00:00,  2.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 113.29 im/s\n",
            "Throughput improvement: 1.34x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tome.patch.timm(model)\n",
        "\n",
        "model.r = 16\n",
        "tome_throughput = tome.utils.benchmark(\n",
        "    model,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        "    runs=runs,\n",
        "    batch_size=batch_size,\n",
        "    input_size=input_size\n",
        ")\n",
        "print(f\"Throughput improvement: {tome_throughput / baseline_throughput:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMf0JRFk-GPp",
        "outputId": "08f054ee-ebeb-4e15-eafb-f463899b45a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Benchmarking: 100%|██████████| 50/50 [01:14<00:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 165.62 im/s\n",
            "Throughput improvement: 1.96x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging layer with a decreasing schedule,  “decreasing” schedule that removes 2r tokens\n",
        "in the first layer and 0 tokens in the last layer, linearly interpolating for the rest"
      ],
      "metadata": {
        "id": "NVGOvcsBWX_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ToMe with r=16 and a decreasing schedule\n",
        "model.r = (16, -1.0)\n",
        "tome_decr_throughput = tome.utils.benchmark(\n",
        "    model,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        "    runs=runs,\n",
        "    batch_size=batch_size,\n",
        "    input_size=input_size\n",
        ")\n",
        "print(f\"Throughput improvement: {tome_decr_throughput / baseline_throughput:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvn5V3GTmK9W",
        "outputId": "eafbd00e-7a75-436a-8fdf-ce8a22b3944e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Benchmarking: 100%|██████████| 50/50 [02:18<00:00,  2.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 83.89 im/s\n",
            "Throughput improvement: 1.00x\n"
          ]
        }
      ]
    }
  ]
}